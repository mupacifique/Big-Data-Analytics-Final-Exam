print("Starting Spark ETL job (Memory Optimized)...")

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, StringType

# ----------------------------
# 1Ô∏è‚É£ Create Spark Session with MORE MEMORY
# ----------------------------
spark = (
    SparkSession.builder
    .appName("FinalProjectETL")
    .config("spark.driver.memory", "16g")           # ‚Üê INCREASED from 4g to 8g
    .config("spark.executor.memory", "16g")         # ‚Üê INCREASED from 4g to 8g
    .config("spark.memory.fraction", "0.8")        # ‚Üê Use 80% of memory for execution
    .config("spark.memory.storageFraction", "0.2") # ‚Üê 20% for storage
    .config("spark.sql.shuffle.partitions", "4")   # ‚Üê REDUCED partitions (less memory per partition)
    .config("spark.default.parallelism", "16")      # ‚Üê Reduce parallelism
    .config("spark.driver.maxResultSize", "16g")    # ‚Üê Increase max result size
    .config("spark.sql.files.maxPartitionBytes", "64m")  # ‚Üê Smaller file partitions
    .getOrCreate()
)

spark.sparkContext.setLogLevel("ERROR")
print("‚úÖ Spark session created")

# ----------------------------
# 2Ô∏è‚É£ Define JSON Schema
# ----------------------------
session_schema = StructType([
    StructField("user_id", StringType(), True),
    StructField("event", StringType(), True),
    StructField("timestamp", StringType(), True)
])

# ----------------------------
# 3Ô∏è‚É£ Load JSON files (with memory optimization)
# ----------------------------
input_path = r"C:\Users\pmuyiringire\OneDrive - Bank of Kigali\BIG DATA ANALYTICS\SEM 3\BIG DATA ANALYTICS\Assignments\Final Project\data\raw_events\*.json"

print(f"üìÇ Loading JSON from: {input_path}")
print("‚è≥ This may take a while for large files...")

try:
    df = (
        spark.read
        .schema(session_schema)
        .option("mode", "DROPMALFORMED")  # Drop bad records instead of failing
        .json(input_path)
    )
    
    print("‚úÖ JSON files loaded successfully")
    
    # DON'T call count() yet - it triggers computation!
    # Instead, just show a sample
    print("\nüìã Sample data (first 5 rows):")
    df.show(5, truncate=False)
    
except Exception as e:
    print(f"‚ùå ERROR loading JSON: {e}")
    spark.stop()
    exit(1)

# ----------------------------
# 4Ô∏è‚É£ Write to Parquet (COALESCE to reduce partitions)
# ----------------------------
output_path = r"C:\Users\pmuyiringire\OneDrive - Bank of Kigali\BIG DATA ANALYTICS\SEM 3\BIG DATA ANALYTICS\Assignments\Final Project\analytics\outputs\sessions.parquet"

print(f"\nüíæ Writing to Parquet: {output_path}")
print("‚è≥ Processing and writing data...")

try:
    # Coalesce to fewer partitions to reduce memory pressure
    # Use 2-4 partitions for datasets < 10GB
    df.coalesce(2).write.mode("overwrite").parquet(output_path)
    
    print("‚úÖ Data written to Parquet successfully!")
    
    # Now verify by reading back
    print("\nüîç Verifying written data...")
    df_verify = spark.read.parquet(output_path)
    row_count = df_verify.count()
    
    print(f"‚úÖ Verification complete: {row_count:,} rows written")
    
    # Show sample of written data
    print("\nüìã Sample from written Parquet:")
    df_verify.show(5, truncate=False)
    
except Exception as e:
    print(f"‚ùå ERROR writing/verifying data: {e}")
    print("\nIf you still get memory errors, try:")
    print("  1. Reduce the size of your JSON files (use fewer events)")
    print("  2. Process files one at a time")
    print("  3. Increase memory further to 12g or 16g")
    spark.stop()
    exit(1)

# ----------------------------
# 5Ô∏è‚É£ Stop Spark
# ----------------------------
spark.stop()
print("\n‚úÖ Spark ETL job finished successfully!")
print("="*60)
print("Next step: python analytics.py")
print("="*60)
